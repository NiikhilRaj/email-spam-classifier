{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c1a1ab",
   "metadata": {},
   "source": [
    "# Email Spam Classifier: Model Evaluation & Streamlit GUI\n",
    "\n",
    "This notebook visualizes the performance of the trained VotingClassifier (MultinomialNB + RandomForest) and provides a Streamlit GUI for interactive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cefbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import streamlit as st\n",
    "import os\n",
    "from project.preprocessing import batch_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model and Vectorizer\n",
    "MODEL_PATH = os.path.join('project', 'final_model.pkl')\n",
    "DATA_PATH = '190k_mails_spam.csv'  # Adjust if needed\n",
    "\n",
    "# Load model and vectorizer\n",
    "model_bundle = joblib.load(MODEL_PATH)\n",
    "model = model_bundle['model']\n",
    "vectorizer = model_bundle['vectorizer']\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "text_col = next((c for c in df.columns if 'text' in c or 'message' in c or 'email' in c), df.columns[0])\n",
    "label_col = next((c for c in df.columns if 'label' in c or 'spam' in c or 'target' in c), df.columns[-1])\n",
    "df = df[[text_col, label_col]].dropna()\n",
    "\n",
    "# Preprocess text\n",
    "df['clean_text'] = batch_preprocess(df[text_col].astype(str).tolist())\n",
    "\n",
    "# 66-22-12 split (same as training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.34, random_state=42, stratify=df[label_col])\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.353, random_state=42, stratify=temp_df[label_col])\n",
    "\n",
    "X_test = vectorizer.transform(test_df['clean_text'])\n",
    "y_test = test_df[label_col]\n",
    "X_val = vectorizer.transform(val_df['clean_text'])\n",
    "y_val = val_df[label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prediction Function\n",
    "def predict_spam(texts):\n",
    "    clean_texts = batch_preprocess(texts)\n",
    "    X = vectorizer.transform(clean_texts)\n",
    "    preds = model.predict(X)\n",
    "    probs = model.predict_proba(X) if hasattr(model, 'predict_proba') else None\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Accuracy and Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "prec_val = precision_score(y_val, y_val_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Set': ['Test', 'Validation'],\n",
    "    'Accuracy': [acc_test, acc_val],\n",
    "    'Precision': [prec_test, prec_val]\n",
    "})\n",
    "\n",
    "metrics_df.set_index('Set')[['Accuracy', 'Precision']].plot(kind='bar', ylim=(0,1), figsize=(7,4), title='Accuracy & Precision')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Test Set Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Validation Set Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Performance (ROC Curve, Precision-Recall Curve)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize labels if not already 0/1\n",
    "if set(y_test.unique()) != {0,1}:\n",
    "    y_test_bin = label_binarize(y_test, classes=[y_test.min(), y_test.max()]).ravel()\n",
    "else:\n",
    "    y_test_bin = y_test\n",
    "\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "else:\n",
    "    y_score = model.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test_bin, y_score)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf308757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on Sample Texts\n",
    "sample_texts = [\n",
    "    \"Congratulations! You've won a free ticket. Reply to claim.\",\n",
    "    \"Hi, can we reschedule our meeting to tomorrow?\",\n",
    "    \"Urgent: Your account will be suspended unless you verify now.\",\n",
    "    \"Lunch at 1pm? Let me know.\"\n",
    "]\n",
    "preds, probs = predict_spam(sample_texts)\n",
    "for text, pred, prob in zip(sample_texts, preds, probs):\n",
    "    print(f\"Text: {text}\\nPrediction: {'Spam' if pred else 'Ham'} | Probability (Spam): {prob[1]:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69580198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit GUI for Prediction\n",
    "st.title('Email Spam Classifier')\n",
    "st.write('Enter your email text below and click Predict to see if it is spam or not.')\n",
    "\n",
    "user_input = st.text_area('Email Text', '')\n",
    "if st.button('Predict'):\n",
    "    if user_input.strip():\n",
    "        pred, prob = predict_spam([user_input])\n",
    "        label = 'Spam' if pred[0] else 'Ham'\n",
    "        st.write(f'**Prediction:** {label}')\n",
    "        if prob is not None:\n",
    "            st.write(f'**Probability (Spam):** {prob[0][1]:.2f}')\n",
    "    else:\n",
    "        st.warning('Please enter some text.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
